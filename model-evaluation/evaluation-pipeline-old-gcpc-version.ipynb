{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07b3a5-5af9-4639-98e0-b25c16c0de79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0fc93-659f-4ddc-ab8c-fe0ec492204b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install google-cloud-pipeline-components==1.0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6648671-757c-443e-9e98-6d636ee2eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTART YOUR KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69c4ae-1f42-4efd-b6a6-05b59f285a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment Variables - Modify accordingly\n",
    "\n",
    "# Project ID\n",
    "PROJECT_ID = \"PROJECT-ID\"\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "\n",
    "# Region/Location e.g. \"us-central1\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Service Account\n",
    "SERVICE_ACCOUNT = \"YOUR-SA\"\n",
    "\n",
    "# === GCS DATA ===\n",
    "\n",
    "# Bucket URI for the evaluation tasks e.g. \"gs://eval-gcs-test3\"\n",
    "BUCKET_URI = \"gs://eval-gcs-test3\"\n",
    "\n",
    "# Dir in bucket for evaluation\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/evaluation_task\"\n",
    "\n",
    "# Bucket URI with the test data\n",
    "GCS_SOURCE_URI = f\"{PIPELINE_ROOT}/test_data_correct_string_format.csv\"\n",
    "\n",
    "# Bucket URI for the result data e.g. \"gs://eval-gcs-test3/result/\"\n",
    "GCS_DESTINATION_OUTPUT_URI = PIPELINE_ROOT\n",
    "\n",
    "# === MODEL DATA ===\n",
    "\n",
    "# Display name for the Vertex AI Model \"my_model\"\n",
    "MODEL_ID_VERSION = \"1712582818749480960@1\"\n",
    "\n",
    "# Model Resource Name in Vertex AI. e.g. projects/PROJECT_ID/locations/REGION/models/MODEL_DISPLAY_NAME\n",
    "MODEL_RESOURCE_NAME = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID_VERSION}\"\n",
    "\n",
    "# Target column in the TEST dataset. e.g. \"target_column\"\n",
    "TARGET = \"income_bracket\"\n",
    "\n",
    "# Class labels for classification batch prediction. e.g. [\"class1\", \"class2\"]\n",
    "CLASS_LABELS = [\"<=50K\", \">50K\"]\n",
    "\n",
    "# === PIPELINE DATA ===\n",
    "\n",
    "# Display name for your Vertex AI Pipeline. e.g. (\"classification_model_evaluation_pipeline\")\n",
    "PIPELINE_DISPLAY_NAME = (\"classification_model_evaluation_pipeline\")\n",
    "\n",
    "# Path where the compiled pipeline needs to be written\n",
    "PIPELINE_PACKAGE_PATH = \"compiled_pipeline.json\"\n",
    "\n",
    "# === DATAFLOW DATA ===\n",
    "\n",
    "DATAFLOW_SERVICE_ACCOUNT = \"\"\n",
    "\n",
    "DATAFLOW_SUBNETWORK = \"\"\n",
    "\n",
    "DATAFLOW_USE_PUBLIC_IP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60018238-d9f8-432b-848f-53cfb66f836f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure service account\n",
    "import sys\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"YOUR-SA\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9cdb4-b557-46c0-a693-2ff9c15d11d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add permissions to Service Account to Bucket\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea89815-acd5-46ca-b108-1b343f373ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Evaluation Pipeline\n",
    "\n",
    "import kfp\n",
    "import json\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"custom-tabular-classification-evaluation-pipeline\")\n",
    "def evaluation_custom_tabular_feature_attribution_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    root_dir: str,\n",
    "    model_name: str,\n",
    "    target_field_name: str,\n",
    "    gcs_source_input_uris: \"JsonArray\",\n",
    "    gcs_destination_output_uri_prefix: str,\n",
    "    batch_predict_instances_format: str,\n",
    "    evaluation_class_names: list,\n",
    "    batch_predict_predictions_format: str = \"jsonl\",\n",
    "    evaluation_prediction_label_column: str = \"\",\n",
    "    evaluation_prediction_score_column: str = \"prediction\",\n",
    "    enable_caching: bool = False,\n",
    "    dataflow_service_account: str = \"\",\n",
    "    dataflow_subnetwork: str = \"\",\n",
    "    dataflow_use_public_ips: bool = True,\n",
    "    batch_predict_machine_type: str = \"n1-standard-4\",\n",
    "    batch_predict_starting_replica_count: int = 5,\n",
    "    batch_predict_max_replica_count: int = 10,\n",
    "    batch_predict_data_sample_size: int = 10000,\n",
    "):\n",
    "    # Import the components\n",
    "    from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "    from google_cloud_pipeline_components.experimental.evaluation import (\n",
    "        EvaluationDataSamplerOp, GetVertexModelOp,\n",
    "        ModelEvaluationClassificationOp, ModelImportEvaluationOp,\n",
    "        TargetFieldDataRemoverOp)\n",
    "\n",
    "    # Get the Vertex AI model resource\n",
    "    get_model_task = GetVertexModelOp(model_resource_name=model_name)\n",
    "\n",
    "    # Run the data sampling task\n",
    "    data_sampler_task = EvaluationDataSamplerOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        root_dir=root_dir,\n",
    "        gcs_source_uris=gcs_source_input_uris,\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        sample_size=batch_predict_data_sample_size,\n",
    "    )\n",
    "\n",
    "    # Run the task to remove the target field from data for batch prediction\n",
    "    data_splitter_task = TargetFieldDataRemoverOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        root_dir=root_dir,\n",
    "        gcs_source_uris=data_sampler_task.outputs['gcs_output_directory'],\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        target_field_name=target_field_name,\n",
    "    )\n",
    "\n",
    "    # Run the batch prediction task\n",
    "    batch_predict_task = ModelBatchPredictOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        job_display_name=\"model-registry-batch-prediction\",\n",
    "        gcs_source_uris=data_splitter_task.outputs['gcs_output_directory'],\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        predictions_format=batch_predict_predictions_format,\n",
    "        gcs_destination_output_uri_prefix=gcs_destination_output_uri_prefix,\n",
    "        machine_type=batch_predict_machine_type,\n",
    "        starting_replica_count=batch_predict_starting_replica_count,\n",
    "        max_replica_count=batch_predict_max_replica_count,\n",
    "    )\n",
    "\n",
    "    # Run the evaluation based on prediction type\n",
    "    eval_task = ModelEvaluationClassificationOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        root_dir=root_dir,\n",
    "        class_labels=evaluation_class_names,\n",
    "        prediction_label_column=evaluation_prediction_label_column,\n",
    "        prediction_score_column=evaluation_prediction_score_column,\n",
    "        target_field_name=target_field_name,\n",
    "        ground_truth_format=batch_predict_instances_format,\n",
    "        ground_truth_gcs_source=data_sampler_task.outputs[\"gcs_output_directory\"],\n",
    "        predictions_format=batch_predict_predictions_format,\n",
    "        predictions_gcs_source=batch_predict_task.outputs[\"gcs_output_directory\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    evaluation_metrics: dsl.Output[google.ClassificationMetrics], target_field_name: str, model: dsl.Input[google.VertexModel] = None, location: str = 'us-central1', predictions_format: str = 'jsonl', predictions_gcs_source: dsl.Input[system.Artifact] = None, predictions_bigquery_source: dsl.Input[google.BQTable] = None, ground_truth_format: str = 'jsonl', ground_truth_gcs_source: list[str] = [], ground_truth_bigquery_source: str = '', classification_type: str = 'multiclass', class_labels: list[str] = [], prediction_score_column: str = '', prediction_label_column: str = '', slicing_specs: list[Any] = [], positive_classes: list[str] = [], dataflow_service_account: str = '', dataflow_disk_size_gb: int = 50, dataflow_machine_type: str = 'n1-standard-4', dataflow_workers_num: int = 1, dataflow_max_workers_num: int = 5, dataflow_subnetwork: str = '', dataflow_use_public_ips: bool = True, encryption_spec_key_name: str = '', force_runner_mode: str = '', project: str = '{{$.pipeline_google_cloud_project_id}}'\n",
    "\n",
    "    # Import the model evaluations to the Vertex AI model\n",
    "    ModelImportEvaluationOp(\n",
    "        classification_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        dataset_type=batch_predict_instances_format,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5268d-89ab-43ba-b7fc-39ec1c72e15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=evaluation_custom_tabular_feature_attribution_pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ce1b7-150f-4042-8e2c-e44cde518c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"root_dir\": PIPELINE_ROOT,\n",
    "    \"model_name\": MODEL_RESOURCE_NAME,\n",
    "    \"target_field_name\": TARGET,\n",
    "    \"evaluation_class_names\": CLASS_LABELS,\n",
    "    \"gcs_source_input_uris\": f\"[\\\"{GCS_SOURCE_URI}\\\"]\",\n",
    "    \"gcs_destination_output_uri_prefix\": GCS_DESTINATION_OUTPUT_URI,\n",
    "    \"dataflow_service_account\": DATAFLOW_SERVICE_ACCOUNT,\n",
    "    \"dataflow_subnetwork\": DATAFLOW_SUBNETWORK,\n",
    "    \"dataflow_use_public_ips\": DATAFLOW_USE_PUBLIC_IP,\n",
    "    \"batch_predict_instances_format\": \"csv\",\n",
    "    \"batch_predict_predictions_format\": \"jsonl\",\n",
    "    \"batch_predict_data_sample_size\": 3000,\n",
    "    \"enable_caching\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe9814-e513-4695-842a-51be894843d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(staging_bucket=PIPELINE_ROOT)\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "# Run the pipeline job\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cc5bb-e831-48b9-85ea-48effb11cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/googleapis/python-aiplatform/blob/cd85d8f74d3922de3f871415bacf77c594f0c547/google/cloud/aiplatform/models.py#L5193\n",
    "\n",
    "#https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.40/google_cloud_pipeline_components.experimental.evaluation.html#google_cloud_pipeline_components.experimental.evaluation.ModelEvaluationClassificationOp\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/official/model_evaluation"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
