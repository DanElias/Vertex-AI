{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605a15c-4b56-4217-9dcf-56f9895e7cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eac55c-d2f0-4f79-9c84-8c6a93ef9d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install google-cloud-pipeline-components==2.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9fa59-7a4d-44e1-b92f-a0275c3071e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment Variables - Modify accordingly\n",
    "\n",
    "# Project ID\n",
    "PROJECT_ID = \"PROJECT-ID\"\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "\n",
    "# Region/Location e.g. \"us-central1\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Service Account\n",
    "SERVICE_ACCOUNT = \"YOUR-SA\"\n",
    "\n",
    "# === GCS DATA ===\n",
    "\n",
    "# Bucket URI for the evaluation tasks e.g. \"gs://eval-gcs-test3\"\n",
    "BUCKET_URI = \"gs://eval-gcs-test3\"\n",
    "\n",
    "# Dir in bucket for evaluation\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/evaluation_task\"\n",
    "\n",
    "# Bucket URI with the test data\n",
    "GCS_SOURCE_URI = f\"{PIPELINE_ROOT}/test_data_correct_string_format.csv\"\n",
    "\n",
    "# Bucket URI for the result data e.g. \"gs://eval-gcs-test3/result/\"\n",
    "GCS_DESTINATION_OUTPUT_URI = PIPELINE_ROOT\n",
    "\n",
    "# === MODEL DATA ===\n",
    "\n",
    "# Display name for the Vertex AI Model \"my_model\"\n",
    "MODEL_ID_VERSION = \"1712582818749480960@1\"\n",
    "\n",
    "# Model Resource Name in Vertex AI. e.g. projects/PROJECT_ID/locations/REGION/models/MODEL_DISPLAY_NAME\n",
    "# MODEL_RESOURCE_NAME = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID_VERSION}\"\n",
    "\n",
    "# Target column in the TEST dataset. e.g. \"target_column\"\n",
    "TARGET = \"income_bracket\"\n",
    "\n",
    "# Class labels for classification batch prediction. e.g. [\"class1\", \"class2\"]\n",
    "#CLASS_LABELS = [\"\\\"\\\"\\\"<=50K\\\"\\\"\\\"\", \"\\\"\\\"\\\">50K\\\"\\\"\\\"\"] -> [\"\\\"\\\"\\\"<=50K\\\"\\\"\\\"\",\"\\\"\\\"\\\">50K\\\"\\\"\\\"\"]\n",
    "#CLASS_LABELS = [\"\"\"<=50K\"\"\", \"\"\">50K\"\"\"] -> [\"<=50K\", \">50K\"]\n",
    "#CLASS_LABELS = ['\"\"\"<=50K\"\"\"','\"\"\">50K\"\"\"'] -> [\"\\\"\\\"\\\"<=50K\\\"\\\"\\\"\",\"\\\"\\\"\\\">50K\\\"\\\"\\\"\"]\n",
    "\n",
    "# === PIPELINE DATA ===\n",
    "\n",
    "# Display name for your Vertex AI Pipeline. e.g. (\"classification_model_evaluation_pipeline\")\n",
    "PIPELINE_DISPLAY_NAME = (\"classification_model_evaluation_pipeline\")\n",
    "\n",
    "# Path where the compiled pipeline needs to be written\n",
    "PIPELINE_PACKAGE_PATH = \"compiled_pipeline.json\"\n",
    "\n",
    "# === DATAFLOW DATA ===\n",
    "\n",
    "DATAFLOW_SERVICE_ACCOUNT = \"\"\n",
    "\n",
    "DATAFLOW_SUBNETWORK = \"\"\n",
    "\n",
    "DATAFLOW_USE_PUBLIC_IP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b8f5c-1389-45ac-8cbb-00dfeb0101e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure service account\n",
    "import sys\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"YOUR-SA\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85e428-3b53-4635-b7bf-003add116abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add permissions to Service Account to Bucket\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a07d8f-8c68-49d0-add9-736d4c9fc753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Evaluation Pipeline\n",
    "\n",
    "import kfp\n",
    "import json\n",
    "\n",
    "from kfp import dsl\n",
    "\n",
    "from google_cloud_pipeline_components.v1.model import ModelGetOp\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "from google_cloud_pipeline_components.v1.model_evaluation import ModelEvaluationClassificationOp\n",
    "from google_cloud_pipeline_components._implementation.model_evaluation import EvaluationDataSamplerOp\n",
    "from google_cloud_pipeline_components._implementation.model_evaluation import ModelImportEvaluationOp\n",
    "from google_cloud_pipeline_components._implementation.model_evaluation import TargetFieldDataRemoverOp\n",
    "\n",
    "@dsl.pipeline(name=\"custom-tabular-classification-evaluation-pipeline-new-version\")\n",
    "def evaluation_custom_tabular_feature_attribution_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_name: str,\n",
    "    target_field_name: str,\n",
    "    gcs_source_input_uris: list,\n",
    "    gcs_destination_output_uri_prefix: str,\n",
    "    batch_predict_instances_format: str,\n",
    "    evaluation_class_names: list,\n",
    "    batch_predict_predictions_format: str = \"jsonl\",\n",
    "    evaluation_prediction_label_column: str = \"\",\n",
    "    evaluation_prediction_score_column: str = \"prediction\",\n",
    "    enable_caching: bool = False,\n",
    "    dataflow_service_account: str = \"\",\n",
    "    dataflow_subnetwork: str = \"\",\n",
    "    dataflow_use_public_ips: bool = True,\n",
    "    batch_predict_machine_type: str = \"n1-standard-4\",\n",
    "    batch_predict_starting_replica_count: int = 5,\n",
    "    batch_predict_max_replica_count: int = 10,\n",
    "    #batch_predict_data_sample_size: int = 10000,\n",
    "):\n",
    "    # Import the components\n",
    "    \n",
    "\n",
    "    # Get the Vertex AI model resource\n",
    "    # https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.14.0/api/v1/model.html#v1.model.ModelGetOp\n",
    "    get_model_task = ModelGetOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Run the data sampling task\n",
    "    #data_sampler_task = EvaluationDataSamplerOp(\n",
    "    #    project=project,\n",
    "    #    location=location,\n",
    "    #    gcs_source_uris=gcs_source_input_uris,\n",
    "    #    instances_format=batch_predict_instances_format,\n",
    "    #    sample_size=batch_predict_data_sample_size,\n",
    "    #)\n",
    "\n",
    "    # Run the task to remove the target field from data for batch prediction\n",
    "    data_splitter_task = TargetFieldDataRemoverOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        gcs_source_uris=gcs_source_input_uris,#data_sampler_task.outputs['gcs_output_directory'],\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        target_field_name=target_field_name,\n",
    "    )\n",
    "\n",
    "    # Run the batch prediction task\n",
    "    # https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.14.0/api/v1/batch_predict_job.html#v1.batch_predict_job.ModelBatchPredictOp\n",
    "    batch_predict_task = ModelBatchPredictOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        job_display_name=\"model-registry-batch-prediction\",\n",
    "        gcs_source_uris=data_splitter_task.outputs['gcs_output_directory'],\n",
    "        instances_format=batch_predict_instances_format,\n",
    "        predictions_format=batch_predict_predictions_format,\n",
    "        gcs_destination_output_uri_prefix=gcs_destination_output_uri_prefix,\n",
    "        machine_type=batch_predict_machine_type,\n",
    "        starting_replica_count=batch_predict_starting_replica_count,\n",
    "        max_replica_count=batch_predict_max_replica_count,\n",
    "    )\n",
    "    \n",
    "    # Run the evaluation based on prediction type\n",
    "    # https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.14.0/api/v1/model_evaluation.html#v1.model_evaluation.ModelEvaluationClassificationOp\n",
    "    eval_task = ModelEvaluationClassificationOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        target_field_name=target_field_name,\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        \n",
    "        predictions_format=batch_predict_predictions_format,\n",
    "        predictions_gcs_source=batch_predict_task.outputs[\"gcs_output_directory\"],\n",
    "        \n",
    "        ground_truth_format=batch_predict_instances_format,\n",
    "        ground_truth_gcs_source=gcs_source_input_uris,#data_sampler_task.outputs[\"gcs_output_directory\"],\n",
    "        \n",
    "        class_labels=evaluation_class_names,\n",
    "        prediction_score_column=evaluation_prediction_score_column,\n",
    "        prediction_label_column=evaluation_prediction_label_column,\n",
    "        dataflow_service_account=dataflow_service_account,\n",
    "        dataflow_subnetwork=dataflow_subnetwork,\n",
    "        dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "        force_runner_mode=\"Dataflow\" # do not change\n",
    "    )\n",
    "\n",
    "    # Import the model evaluations to the Vertex AI model\n",
    "    ModelImportEvaluationOp(\n",
    "        classification_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "        model=get_model_task.outputs[\"model\"],\n",
    "        dataset_type=batch_predict_instances_format,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc3f8e-5bd8-45d3-a3c8-53daed1b60b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=evaluation_custom_tabular_feature_attribution_pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f5f18-5343-4712-9fc3-da6b487f5352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"model_name\": MODEL_ID_VERSION,\n",
    "    \"target_field_name\": TARGET,\n",
    "    \"evaluation_class_names\": CLASS_LABELS,\n",
    "    \"gcs_source_input_uris\": [GCS_SOURCE_URI],\n",
    "    \"gcs_destination_output_uri_prefix\": GCS_DESTINATION_OUTPUT_URI,\n",
    "    \"dataflow_service_account\": DATAFLOW_SERVICE_ACCOUNT,\n",
    "    \"dataflow_subnetwork\": DATAFLOW_SUBNETWORK,\n",
    "    \"dataflow_use_public_ips\": DATAFLOW_USE_PUBLIC_IP,\n",
    "    \"batch_predict_instances_format\": \"csv\",\n",
    "    \"batch_predict_predictions_format\": \"jsonl\",\n",
    "    #\"batch_predict_data_sample_size\": 3000,\n",
    "    \"enable_caching\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55249887-ddd6-40d7-87ac-55311d64df7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(staging_bucket=PIPELINE_ROOT)\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "# Run the pipeline job\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace16d2-6b1e-47ae-9b14-a8a2d188566a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
